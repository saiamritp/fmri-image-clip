{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def list_get_all_index(list, value):\n",
    "    return [i for i, v in enumerate(list) if v == value]\n",
    "\n",
    "\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    # pad the last dimension only\n",
    "    padding_config = [(0,0)] * (x.ndim - 1) + [(0, patch_size-x.shape[-1]%patch_size)]\n",
    "    return np.pad(x, padding_config, 'wrap')\n",
    "\n",
    "def get_stimuli_list(root, sub):\n",
    "    sti_name = []\n",
    "    path = os.path.join(root, 'Stimuli_Presentation_Lists', sub)\n",
    "    folders = os.listdir(path)\n",
    "    folders.sort()\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(os.path.join(path, folder)):\n",
    "            continue\n",
    "        files = os.listdir(os.path.join(path, folder))\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                sti_name += list(np.loadtxt(os.path.join(path, folder, file), dtype=str))\n",
    "\n",
    "    sti_name_to_return = []\n",
    "    for name in sti_name:\n",
    "        if name.startswith('rep_'):\n",
    "            name = name.replace('rep_', '', 1)\n",
    "        sti_name_to_return.append(name)\n",
    "    return sti_name_to_return\n",
    "\n",
    "def pad_fmri_to_target(fmri_data, target_samples, target_voxels):\n",
    "    # Padding for the first dimension (samples)\n",
    "    if fmri_data.shape[0] < target_samples:\n",
    "        sample_pad_size = target_samples - fmri_data.shape[0]\n",
    "        fmri_data = np.pad(fmri_data, ((0, sample_pad_size), (0, 0)), mode='constant')\n",
    "    elif fmri_data.shape[0] > target_samples:\n",
    "        fmri_data = fmri_data[:target_samples, :]  # Crop to target sample size\n",
    "\n",
    "    # Padding for the second dimension (voxels)\n",
    "    if fmri_data.shape[1] < target_voxels:\n",
    "        voxel_pad_size = target_voxels - fmri_data.shape[1]\n",
    "        fmri_data = np.pad(fmri_data, ((0, 0), (0, voxel_pad_size)), mode='constant')\n",
    "    elif fmri_data.shape[1] > target_voxels:\n",
    "        fmri_data = fmri_data[:, :target_voxels]  # Crop to target voxel size\n",
    "\n",
    "    return fmri_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BOLD5000_Contrastive_Dataset(Dataset):\n",
    "    def __init__(self, fmri, image, fmri_transform=None, image_transform=None, num_voxels=0):\n",
    "        self.fmri = torch.tensor(fmri)\n",
    "        self.image = image\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        \n",
    "        self.image = np.transpose(image, (0, 3, 1, 2)) \n",
    "        self.image = self.image.astype(np.float32) / 255.0\n",
    "        \n",
    "        self.resnet_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) \n",
    "        self.resnet_model.eval()\n",
    "        self.resnet_model = torch.nn.Sequential(*list(self.resnet_model.children())[:-1])\n",
    "\n",
    "        # self.image_encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")  \n",
    "        # self.image_encoder = torch.quantization.quantize_dynamic(self.image_encoder, dtype=torch.qint8)\n",
    "        # self.image_embeddings = self.image_encoder.get_image_features(pixel_values=inputs)\n",
    "\n",
    "        inputs = torch.from_numpy(self.image)\n",
    "        self.image_embeddings = self.resnet_model(inputs).squeeze(-1).squeeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fmri = self.fmri[idx]\n",
    "        img = self.image[idx]\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            label = 1\n",
    "            return {'fmri': fmri, 'image': img, 'label': torch.tensor(label)}\n",
    "        else:\n",
    "            dissimilar_idx1 = np.random.randint(0, len(self.fmri))\n",
    "            dissimilar_idx2 = np.random.randint(0, len(self.fmri))\n",
    "            fmri_dissimilar = self.fmri[dissimilar_idx1]\n",
    "            img_dissimilar = self.image[dissimilar_idx2]\n",
    "\n",
    "            label = 0\n",
    "            return {'fmri': fmri_dissimilar, 'image': img_dissimilar, 'label': torch.tensor(label)}\n",
    "\n",
    "def create_BOLD5000_dataset(path='../data/BOLD5000', patch_size=16, fmri_transform=None,\n",
    "                            image_transform=None, subjects=['CSI1', 'CSI2', 'CSI3', 'CSI4'], \n",
    "                            include_nonavg_test=False, include_image_caption=False, include_image_clip=False):\n",
    "    roi_list = ['EarlyVis', 'LOC', 'OPA', 'PPA', 'RSC']\n",
    "    fmri_path = os.path.join(path, 'BOLD5000_GLMsingle_ROI_betas/py')\n",
    "    img_path = os.path.join(path, 'BOLD5000_Stimuli')\n",
    "    imgs_dict = np.load(os.path.join(img_path, 'Scene_Stimuli/Presented_Stimuli/img_dict.npy'), allow_pickle=True).item()\n",
    "    \n",
    "    repeated_imgs_list = np.loadtxt(os.path.join(img_path, 'Scene_Stimuli', 'repeated_stimuli_113_list.txt'), dtype=str)\n",
    "\n",
    "    fmri_files = [f for f in os.listdir(fmri_path) if f.endswith('.npy')]\n",
    "    fmri_files.sort()\n",
    "\n",
    "    fmri_train_major = []\n",
    "    fmri_test_major = []\n",
    "    img_train_major = []\n",
    "    img_test_major = []\n",
    "\n",
    "    for sub in subjects:\n",
    "        fmri_data_sub = []\n",
    "        for roi in roi_list:\n",
    "            for npy in fmri_files:\n",
    "                if npy.endswith('.npy') and sub in npy and roi in npy:\n",
    "                    fmri_data_sub.append(np.load(os.path.join(fmri_path, npy)))\n",
    "        fmri_data_sub = np.concatenate(fmri_data_sub, axis=-1) \n",
    "        fmri_data_sub = normalize(pad_to_patch_size(fmri_data_sub, patch_size))\n",
    "      \n",
    "        img_files = get_stimuli_list(img_path, sub)\n",
    "        img_data_sub = [imgs_dict[name] for name in img_files]\n",
    "\n",
    "        test_idx = [list_get_all_index(img_files, img) for img in repeated_imgs_list]\n",
    "        test_idx = [i for i in test_idx if len(i) > 0]  \n",
    "        test_fmri = np.stack([fmri_data_sub[idx].mean(axis=0) for idx in test_idx])\n",
    "        test_img = np.stack([img_data_sub[idx[0]] for idx in test_idx])\n",
    "\n",
    "        test_idx_flatten = []\n",
    "        for idx in test_idx:\n",
    "            test_idx_flatten += idx \n",
    "        if include_nonavg_test:\n",
    "            test_fmri = np.concatenate([test_fmri, fmri_data_sub[test_idx_flatten]], axis=0)\n",
    "            test_img = np.concatenate([test_img, np.stack([img_data_sub[idx] for idx in test_idx_flatten])], axis=0)\n",
    "\n",
    "        train_idx = [i for i in range(len(img_files)) if i not in test_idx_flatten]\n",
    "        train_img = np.stack([img_data_sub[idx] for idx in train_idx])\n",
    "        train_fmri = fmri_data_sub[train_idx]\n",
    "        train_fmri = pad_fmri_to_target(train_fmri, target_samples=4803, target_voxels=512)\n",
    "        test_fmri = pad_fmri_to_target(test_fmri, target_samples=4803, target_voxels=512)\n",
    "\n",
    "        fmri_train_major.append(train_fmri)\n",
    "        fmri_test_major.append(test_fmri)\n",
    "        img_train_major.append(train_img)\n",
    "        img_test_major.append(test_img)\n",
    "\n",
    "    fmri_train_major = np.concatenate(fmri_train_major, axis=0)[:100,:]\n",
    "    fmri_test_major = np.concatenate(fmri_test_major, axis=0)[:100,:]\n",
    "    img_train_major = np.concatenate(img_train_major, axis=0)[:100,:]\n",
    "    img_test_major = np.concatenate(img_test_major, axis=0)[:100,:]\n",
    "\n",
    "    num_voxels = fmri_train_major.shape[-1]\n",
    "    \n",
    "    # fmri_enc = FMRITransformerEncoder(cfg)  # Initialize the fMRI transformer encoder\n",
    "    \n",
    "    return (BOLD5000_Contrastive_Dataset(\n",
    "                fmri_train_major, img_train_major, fmri_transform=fmri_transform, image_transform=image_transform, num_voxels=num_voxels), \n",
    "            BOLD5000_Contrastive_Dataset(\n",
    "                fmri_test_major, img_test_major, fmri_transform=fmri_transform, image_transform=image_transform, num_voxels=num_voxels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FMRIEncoderDecoder(nn.Module):\n",
    "    def __init__(self, cfg: dict):\n",
    "        super(FMRIEncoderDecoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Encoder\n",
    "        max_seq_len = max(cfg['model']['max_seq_len'], cfg['model']['hidden_size'])\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, cfg['model']['hidden_size'])\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg['model']['hidden_size'],\n",
    "            nhead=cfg['model']['num_attention_heads'],\n",
    "            dim_feedforward=cfg['model']['dim_feedforward'],\n",
    "            activation=cfg['model']['activation'],\n",
    "            dropout=cfg['model']['dropout'],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg['model']['num_layers'])\n",
    "\n",
    "        # Decoder\n",
    "        dec_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=cfg['model']['hidden_size'],\n",
    "            nhead=cfg['model']['num_attention_heads'],\n",
    "            dim_feedforward=cfg['model']['dim_feedforward'],\n",
    "            activation=cfg['model']['activation'],\n",
    "            dropout=cfg['model']['dropout'],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=cfg['model']['num_layers'])\n",
    "\n",
    "        self.fc = nn.Linear(cfg['model']['hidden_size'], 512)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_voxels = x.size()\n",
    "\n",
    "        # Encoder\n",
    "        x = x.unsqueeze(1)  \n",
    "        positional_embedding = self.positional_embedding(torch.arange(num_voxels, device=x.device)).unsqueeze(0)\n",
    "        positional_embedding = positional_embedding.expand(batch_size, num_voxels, -1)\n",
    "        x = x + positional_embedding\n",
    "        memory = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.decoder(memory, memory) \n",
    "        x = torch.mean(x, dim=1) \n",
    "        x = self.fc(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = '/home2/prateekj/tether-assgn/notebooks/fmri_encoder_decoder.pth'\n",
    "\n",
    "cfg = {\n",
    "    'model': {\n",
    "        'max_seq_len': 100,\n",
    "        'hidden_size': 512,\n",
    "        'num_attention_heads': 8,\n",
    "        'dim_feedforward': 2048,\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.1,\n",
    "        'num_layers': 6\n",
    "    }\n",
    "}\n",
    "fmri_encoder_decoder = FMRIEncoderDecoder(cfg).to(device)\n",
    "fmri_encoder_decoder.load_state_dict(torch.load(model_path))\n",
    "fmri_encoder_decoder.eval() \n",
    "\n",
    "fmri_encoder = fmri_encoder_decoder.encoder\n",
    "\n",
    "fmri_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/prateekj/miniconda3/envs/bold5000_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home2/prateekj/miniconda3/envs/bold5000_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet_model = models.resnet18(pretrained=True)\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1]) \n",
    "resnet_model.to(device)\n",
    "resnet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FMRIImageAlignModel(nn.Module):\n",
    "    def __init__(self, fmri_encoder, output_dim=512):\n",
    "        super(FMRIImageAlignModel, self).__init__()\n",
    "        self.fmri_encoder = fmri_encoder  \n",
    "        self.fc_layer = nn.Linear(512, output_dim)  \n",
    "\n",
    "    def forward(self, fmri_data):\n",
    "        fmri_embedding = self.fmri_encoder(fmri_data)\n",
    "        return self.fc_layer(fmri_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'model': {\n",
    "        'max_seq_len': 512,  \n",
    "        'hidden_size': 512, \n",
    "        'num_attention_heads': 8,\n",
    "        'dim_feedforward': 2048,\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.1,\n",
    "        'num_layers': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(anchor, positive, label, margin=1.0):\n",
    "    cosine_similarity = torch.nn.functional.cosine_similarity(anchor, positive)\n",
    "    loss = (1 - label) * torch.clamp(margin - cosine_similarity, min=0) + label * (1 - cosine_similarity)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 25/25 [00:07<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 25/25 [00:05<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 25/25 [00:05<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 25/25 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 25/25 [00:05<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.2184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 25/25 [00:05<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 25/25 [00:05<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.2151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 25/25 [00:05<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 25/25 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 25/25 [00:05<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 25/25 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 25/25 [00:05<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 25/25 [00:05<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.2142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 25/25 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.2156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 25/25 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 25/25 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.2173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 25/25 [00:05<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.2170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 25/25 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.2164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 25/25 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 25/25 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.2167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 25/25 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.2125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 25/25 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.2180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 25/25 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 25/25 [00:01<00:00, 17.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2310\n"
     ]
    }
   ],
   "source": [
    "fmri_encoder_decoder = FMRIEncoderDecoder(cfg).to(device)\n",
    "fmri_encoder_decoder.load_state_dict(torch.load('/home2/prateekj/tether-assgn/notebooks/fmri_encoder_decoder.pth'))\n",
    "fmri_encoder_decoder.eval()\n",
    "\n",
    "data_path = '/home2/prateekj/tether-assgn/data/BOLD5000'\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset, test_dataset = create_BOLD5000_dataset(\n",
    "    path=data_path,\n",
    "    patch_size=16,\n",
    "    fmri_transform=lambda x: torch.tensor(x, dtype=torch.float32),\n",
    "    image_transform=image_transform,\n",
    "    subjects=['CSI1', 'CSI2', 'CSI3', 'CSI4'],\n",
    "    include_image_clip=False  \n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "model = FMRIImageAlignModel(fmri_encoder_decoder).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs-contrast')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/25\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        fmri_data = batch['fmri'].to(device)\n",
    "        image_data = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_embedding = resnet_model(image_data).squeeze(3).squeeze(2)\n",
    "\n",
    "        fmri_embedding = model.fmri_encoder(fmri_data)\n",
    "        fmri_embedding_aligned = model(fmri_embedding)\n",
    "\n",
    "        loss = contrastive_loss(fmri_embedding_aligned, image_embedding, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()  \n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad(): \n",
    "        for batch in tqdm(test_loader, desc=\"Validation\"):\n",
    "            fmri_data = batch['fmri'].to(device)\n",
    "            image_data = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            image_embedding = resnet_model(image_data).squeeze(3).squeeze(2)\n",
    "            fmri_embedding = model.fmri_encoder(fmri_data)\n",
    "\n",
    "            fmri_embedding_aligned = model(fmri_embedding)\n",
    "            loss = contrastive_loss(fmri_embedding_aligned, image_embedding, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(test_loader)\n",
    "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fmri_image_align_model.pth\")\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bold5000_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
