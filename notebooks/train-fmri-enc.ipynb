{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FMRIEncoderDecoder(nn.Module):\n",
    "    def __init__(self, cfg: dict):\n",
    "        super(FMRIEncoderDecoder, self).__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Encoder\n",
    "        max_seq_len = max(cfg['model']['max_seq_len'], cfg['model']['hidden_size'])\n",
    "        self.positional_embedding = nn.Embedding(max_seq_len, cfg['model']['hidden_size'])\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=cfg['model']['hidden_size'],\n",
    "            nhead=cfg['model']['num_attention_heads'],\n",
    "            dim_feedforward=cfg['model']['dim_feedforward'],\n",
    "            activation=cfg['model']['activation'],\n",
    "            dropout=cfg['model']['dropout'],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=cfg['model']['num_layers'])\n",
    "\n",
    "        # Decoder\n",
    "        dec_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=cfg['model']['hidden_size'],\n",
    "            nhead=cfg['model']['num_attention_heads'],\n",
    "            dim_feedforward=cfg['model']['dim_feedforward'],\n",
    "            activation=cfg['model']['activation'],\n",
    "            dropout=cfg['model']['dropout'],\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=cfg['model']['num_layers'])\n",
    "\n",
    "        self.fc = nn.Linear(cfg['model']['hidden_size'], 512)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_voxels = x.size()\n",
    "\n",
    "        # Encoder\n",
    "        x = x.unsqueeze(1)  # Add sequence dimension: [batch_size, 1, num_voxels]\n",
    "        positional_embedding = self.positional_embedding(torch.arange(num_voxels, device=x.device)).unsqueeze(0)\n",
    "        positional_embedding = positional_embedding.expand(batch_size, num_voxels, -1)\n",
    "        x = x + positional_embedding\n",
    "        memory = self.encoder(x)\n",
    "\n",
    "        # Decoder (identity: decoding back the same representation)\n",
    "        x = self.decoder(memory, memory)  # Use memory itself as the input to the decoder\n",
    "        x = torch.mean(x, dim=1)  # Average pooling over the sequence dimension\n",
    "        x = self.fc(x)  # Map to 512-d feature vector\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def list_get_all_index(list, value):\n",
    "    return [i for i, v in enumerate(list) if v == value]\n",
    "\n",
    "\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    # pad the last dimension only\n",
    "    padding_config = [(0,0)] * (x.ndim - 1) + [(0, patch_size-x.shape[-1]%patch_size)]\n",
    "    return np.pad(x, padding_config, 'wrap')\n",
    "\n",
    "def get_stimuli_list(root, sub):\n",
    "    sti_name = []\n",
    "    path = os.path.join(root, 'Stimuli_Presentation_Lists', sub)\n",
    "    folders = os.listdir(path)\n",
    "    folders.sort()\n",
    "    for folder in folders:\n",
    "        if not os.path.isdir(os.path.join(path, folder)):\n",
    "            continue\n",
    "        files = os.listdir(os.path.join(path, folder))\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                sti_name += list(np.loadtxt(os.path.join(path, folder, file), dtype=str))\n",
    "\n",
    "    sti_name_to_return = []\n",
    "    for name in sti_name:\n",
    "        if name.startswith('rep_'):\n",
    "            name = name.replace('rep_', '', 1)\n",
    "        sti_name_to_return.append(name)\n",
    "    return sti_name_to_return\n",
    "\n",
    "def pad_fmri_to_target(fmri_data, target_samples, target_voxels):\n",
    "    # Padding for the first dimension (samples)\n",
    "    if fmri_data.shape[0] < target_samples:\n",
    "        sample_pad_size = target_samples - fmri_data.shape[0]\n",
    "        fmri_data = np.pad(fmri_data, ((0, sample_pad_size), (0, 0)), mode='constant')\n",
    "    elif fmri_data.shape[0] > target_samples:\n",
    "        fmri_data = fmri_data[:target_samples, :]  # Crop to target sample size\n",
    "\n",
    "    # Padding for the second dimension (voxels)\n",
    "    if fmri_data.shape[1] < target_voxels:\n",
    "        voxel_pad_size = target_voxels - fmri_data.shape[1]\n",
    "        fmri_data = np.pad(fmri_data, ((0, 0), (0, voxel_pad_size)), mode='constant')\n",
    "    elif fmri_data.shape[1] > target_voxels:\n",
    "        fmri_data = fmri_data[:, :target_voxels]  # Crop to target voxel size\n",
    "\n",
    "    return fmri_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPModel\n",
    "from torchvision import transforms, models\n",
    "\n",
    "class BOLD5000_ResNet_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, fmri_transform=identity, image_transform=identity, num_voxels=0, fmri_encoder=None):\n",
    "        # self.fmri = fmri\n",
    "        self.fmri = torch.tensor(fmri)\n",
    "        self.image = image\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        self.image = np.transpose(image, (0, 3, 1, 2))  \n",
    "        self.image = self.image.astype(np.float32) / 255.0\n",
    "        \n",
    "        self.resnet_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1) \n",
    "        self.resnet_model.eval()\n",
    "        self.resnet_model = torch.nn.Sequential(*list(self.resnet_model.children())[:-1])\n",
    "        \n",
    "        inputs = torch.from_numpy(self.image)\n",
    "        self.image_embeddings = self.resnet_model(inputs).squeeze(-1).squeeze(-1) \n",
    "        \n",
    "        # print(f\"Image embeddings shape: {self.image_embeddings.shape}\")\n",
    "        \n",
    "        self.fmri_encoder = fmri_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fmri = self.fmri[idx]\n",
    "        fmri = self.fmri_transform(fmri)\n",
    "        \n",
    "        fmri_embedding = self.fmri_encoder(fmri.unsqueeze(0)) \n",
    "        \n",
    "        return {\n",
    "            'fmri': fmri_embedding, \n",
    "            'image': self.image_embeddings[idx].unsqueeze(0) \n",
    "        }\n",
    "\n",
    "def create_BOLD5000_dataset(path='../data/BOLD5000', patch_size=16, fmri_transform=None,\n",
    "                            image_transform=None, subjects=['CSI1', 'CSI2', 'CSI3', 'CSI4'], \n",
    "                            include_nonavg_test=False, include_image_caption=False, include_image_clip=False):\n",
    "    roi_list = ['EarlyVis', 'LOC', 'OPA', 'PPA', 'RSC']\n",
    "    fmri_path = os.path.join(path, 'BOLD5000_GLMsingle_ROI_betas/py')\n",
    "    img_path = os.path.join(path, 'BOLD5000_Stimuli')\n",
    "    imgs_dict = np.load(os.path.join(img_path, 'Scene_Stimuli/Presented_Stimuli/img_dict.npy'), allow_pickle=True).item()\n",
    "    \n",
    "    repeated_imgs_list = np.loadtxt(os.path.join(img_path, 'Scene_Stimuli', 'repeated_stimuli_113_list.txt'), dtype=str)\n",
    "\n",
    "    fmri_files = [f for f in os.listdir(fmri_path) if f.endswith('.npy')]\n",
    "    fmri_files.sort()\n",
    "\n",
    "    fmri_train_major = []\n",
    "    fmri_test_major = []\n",
    "    img_train_major = []\n",
    "    img_test_major = []\n",
    "    caption_train_major = []\n",
    "    caption_test_major = []\n",
    "\n",
    "    for sub in subjects:\n",
    "        fmri_data_sub = []\n",
    "        for roi in roi_list:\n",
    "            for npy in fmri_files:\n",
    "                if npy.endswith('.npy') and sub in npy and roi in npy:\n",
    "                    fmri_data_sub.append(np.load(os.path.join(fmri_path, npy)))\n",
    "        fmri_data_sub = np.concatenate(fmri_data_sub, axis=-1)  # concatenate all rois\n",
    "        fmri_data_sub = normalize(pad_to_patch_size(fmri_data_sub, patch_size))\n",
    "      \n",
    "        img_files = get_stimuli_list(img_path, sub)\n",
    "        img_data_sub = [imgs_dict[name] for name in img_files]\n",
    "        # print(\"Image data sub shape: \", np.shape(img_data_sub))\n",
    "\n",
    "        test_idx = [list_get_all_index(img_files, img) for img in repeated_imgs_list]\n",
    "        test_idx = [i for i in test_idx if len(i) > 0]  # remove empty list for CSI4\n",
    "        test_fmri = np.stack([fmri_data_sub[idx].mean(axis=0) for idx in test_idx])\n",
    "        test_img = np.stack([img_data_sub[idx[0]] for idx in test_idx])\n",
    "\n",
    "        test_idx_flatten = []\n",
    "        for idx in test_idx:\n",
    "            test_idx_flatten += idx  \n",
    "        if include_nonavg_test:\n",
    "            test_fmri = np.concatenate([test_fmri, fmri_data_sub[test_idx_flatten]], axis=0)\n",
    "            test_img = np.concatenate([test_img, np.stack([img_data_sub[idx] for idx in test_idx_flatten])], axis=0)\n",
    "\n",
    "        train_idx = [i for i in range(len(img_files)) if i not in test_idx_flatten]\n",
    "        train_img = np.stack([img_data_sub[idx] for idx in train_idx])\n",
    "        train_fmri = fmri_data_sub[train_idx]\n",
    "        train_fmri = pad_fmri_to_target(train_fmri, target_samples=4803, target_voxels=512)\n",
    "        test_fmri = pad_fmri_to_target(test_fmri, target_samples=4803, target_voxels=512)\n",
    "\n",
    "        fmri_train_major.append(train_fmri)\n",
    "        fmri_test_major.append(test_fmri)\n",
    "        img_train_major.append(train_img)\n",
    "        img_test_major.append(test_img)\n",
    "\n",
    "    fmri_train_major = np.concatenate(fmri_train_major, axis=0)[:64,:]\n",
    "    fmri_test_major = np.concatenate(fmri_test_major, axis=0)[:64,:]\n",
    "    img_train_major = np.concatenate(img_train_major, axis=0)[:64,:]\n",
    "    img_test_major = np.concatenate(img_test_major, axis=0)[:64,:]\n",
    "\n",
    "    # print(img_train_major.shape)\n",
    "\n",
    "    num_voxels = fmri_train_major.shape[-1]\n",
    "    \n",
    "    # fmri_enc = FMRITransformerEncoder(cfg)  # Initialize the fMRI transformer encoder\n",
    "    \n",
    "    \n",
    "    if include_image_clip:  \n",
    "        return (BOLD5000_ResNet_dataset(\n",
    "                    fmri_train_major, img_train_major, fmri_transform, image_transform, num_voxels, fmri_encoder=fmri_enc), \n",
    "                BOLD5000_ResNet_dataset(\n",
    "                    fmri_test_major, img_test_major, fmri_transform, image_transform, num_voxels, fmri_encoder=fmri_enc))\n",
    "    else:  # Use BOLD5000_dataset if include_image_clip is False\n",
    "        return (BOLD5000_dataset(fmri_train_major, img_train_major, fmri_transform, image_transform, num_voxels),\n",
    "                BOLD5000_dataset(fmri_test_major, img_test_major, fmri_transform, image_transform, num_voxels))\n",
    "           \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class BOLD5000_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, fmri_transform=identity, image_transform=identity, num_voxels=0):\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        fmri = self.fmri[index]\n",
    "        img = self.image[index] / 255.0  \n",
    "        fmri = np.expand_dims(fmri, axis=0)\n",
    "        \n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = Image.fromarray(np.uint8(img))\n",
    "        \n",
    "        fmri = self.fmri_transform(fmri) \n",
    "        img = self.image_transform(img) \n",
    "        return {'fmri': fmri, 'image': img}\n",
    "\n",
    "\n",
    "class BOLD5000_CLIP_dataset(Dataset):\n",
    "    def __init__(self, fmri, image, fmri_transform=identity, image_transform=identity, num_voxels=0, fmri_encoder=None):\n",
    "        self.fmri = fmri\n",
    "        self.image = image\n",
    "        self.fmri_transform = fmri_transform\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = num_voxels\n",
    "        \n",
    "        resize_transform = transforms.Resize((224, 224)) \n",
    "        self.image = np.transpose(image, (0, 3, 1, 2))  \n",
    "        self.image = np.array([resize_transform(torch.from_numpy(img)) for img in self.image])\n",
    "\n",
    "        self.image = self.image.astype(np.float32) / 255.0\n",
    "    \n",
    "        self.image_encoder = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")  \n",
    "        \n",
    "        self.image_encoder = torch.quantization.quantize_dynamic(self.image_encoder, dtype=torch.qint8)\n",
    "        \n",
    "        inputs = torch.from_numpy(self.image)\n",
    "        self.image_embeddings = self.image_encoder.get_image_features(pixel_values=inputs)\n",
    "        self.fmri_encoder = fmri_encoder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fmri)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fmri = self.fmri[idx]\n",
    "        fmri = self.fmri_transform(fmri)\n",
    "        \n",
    "        fmri_embedding = self.fmri_encoder(fmri.unsqueeze(0))  \n",
    "        \n",
    "        img = self.image[idx]\n",
    "        img = self.image_transform(img) \n",
    "        \n",
    "        return {\n",
    "            'fmri': fmri_embedding, \n",
    "            'image': self.image_embeddings[idx] \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:   0%|          | 0/16 [00:00<?, ?it/s]/home2/prateekj/miniconda3/envs/bold5000_env/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4, 1, 512])) that is different to the input size (torch.Size([4, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/25: 100%|██████████| 16/16 [00:06<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.7727, Validation Loss: 0.4032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 16/16 [00:01<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6651, Validation Loss: 0.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 16/16 [00:01<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.6583, Validation Loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 16/16 [00:01<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.6581, Validation Loss: 0.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 16/16 [00:01<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.6600, Validation Loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 16/16 [00:01<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.6557, Validation Loss: 0.3765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 16/16 [00:01<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.6535, Validation Loss: 0.3777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 16/16 [00:01<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.6565, Validation Loss: 0.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 16/16 [00:01<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.6544, Validation Loss: 0.3839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 16/16 [00:02<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.6611, Validation Loss: 0.3738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 16/16 [00:01<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 0.6556, Validation Loss: 0.3834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 16/16 [00:01<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 0.6541, Validation Loss: 0.3739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 16/16 [00:01<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 0.6532, Validation Loss: 0.3768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 16/16 [00:01<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 0.6533, Validation Loss: 0.3804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 16/16 [00:01<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 0.6522, Validation Loss: 0.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25: 100%|██████████| 16/16 [00:01<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 0.6525, Validation Loss: 0.3776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/25: 100%|██████████| 16/16 [00:01<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss: 0.6514, Validation Loss: 0.3790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/25: 100%|██████████| 16/16 [00:01<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss: 0.6490, Validation Loss: 0.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/25: 100%|██████████| 16/16 [00:01<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss: 0.6483, Validation Loss: 0.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/25: 100%|██████████| 16/16 [00:01<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss: 0.6474, Validation Loss: 0.3742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/25: 100%|██████████| 16/16 [00:02<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss: 0.6502, Validation Loss: 0.3783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/25: 100%|██████████| 16/16 [00:01<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss: 0.6454, Validation Loss: 0.3728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/25: 100%|██████████| 16/16 [00:01<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss: 0.6477, Validation Loss: 0.3754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/25: 100%|██████████| 16/16 [00:01<00:00,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss: 0.6473, Validation Loss: 0.3757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/25: 100%|██████████| 16/16 [00:01<00:00,  8.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss: 0.6483, Validation Loss: 0.3785\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./logs')  \n",
    "\n",
    "data_path = '/home2/prateekj/tether-assgn/data/BOLD5000' \n",
    "from torchvision import transforms\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_dataset, test_dataset = create_BOLD5000_dataset(\n",
    "    path=data_path,\n",
    "    patch_size=16,\n",
    "    fmri_transform=lambda x: torch.tensor(x, dtype=torch.float32),\n",
    "    image_transform=image_transform,\n",
    "    subjects=['CSI1', 'CSI2', 'CSI3', 'CSI4'],\n",
    "    include_nonavg_test=False,\n",
    "    include_image_clip=False\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 4\n",
    "epochs = 25\n",
    "learning_rate = 1e-4\n",
    "cfg = {\n",
    "    'model': {\n",
    "        'max_seq_len': 100,\n",
    "        'hidden_size': 512,\n",
    "        'num_attention_heads': 8,\n",
    "        'dim_feedforward': 2048,\n",
    "        'activation': 'relu',\n",
    "        'dropout': 0.1,\n",
    "        'num_layers': 6\n",
    "    }\n",
    "}\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = FMRIEncoderDecoder(cfg).to(device) \n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        fmri_data = batch['fmri'].to(device) \n",
    "        output = model(fmri_data.squeeze(1))\n",
    "        loss = loss_fn(output, fmri_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            fmri_data = batch['fmri'].to(device)\n",
    "\n",
    "            output = model(fmri_data.squeeze(1))\n",
    "            loss = loss_fn(output, fmri_data)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    writer.add_scalar('Loss/val', avg_val_loss, epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"fmri_encoder_decoder.pth\")\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bold5000_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
